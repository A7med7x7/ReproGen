services:

  jupyter:
    build:
      context: .
      dockerfile:
        {% if "pytorch" in ml_framework and "nvidia" in gpu_type %}
        Dockerfile.jupyter-cuda-pt
        {% elif "pytorch" in ml_framework and "amd" in gpu_type %}
        Dockerfile.jupyter-rocm-pt
        {% elif "tensorflow" in ml_framework and "nvidia" in gpu_type %}
        Dockerfile.jupyter-cuda-tf
        {% elif "tensorflow" in ml_framework and "amd" in gpu_type %}
        Dockerfile.jupyter-rocm-tf
        {% elif "sklearn" in ml_framework %}
        Dockerfile.jupyter-ds
        {% else %}
        Dockerfile.jupyter-cuda-pt
        {% endif %}
    container_name: jupyter
    ports: 
      - "8888:8888"
    volumes:
      - ~/{{ project_name }}:/home/jovyan/work/{{ project_name }}
      - /mnt/data:/home/jovyan/data        
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - PYTHONPATH=/home/jovyan/work/{{ project_name }}
      - MLFLOW_TRACKING_URI=http://mlflow:8000
      {% if include_huggingface %}
      - HF_HOME=${HF_HOME}
      - HF_TOKEN_PATH=${HF_TOKEN_PATH}
      {% endif %}
    {% if "nvidia" in gpu_type %}
    runtime: nvidia
    {% elif "amd" in gpu_type %}
    runtime: rocm
    {% endif %}
    restart: always 
    shm_size: 2g

  mlflow:
    build:
       context: . 
       dockerfile: Dockerfile.mlflow
    container_name: mlflow
    restart: always
    ports:
      - "8000:8000"
    environment:
      - MLFLOW_S3_ENDPOINT_URL=${S3_ENDPOINT_URL}
      - AWS_ACCESS_KEY_ID=${S3_ACCESS_KEY}
      - AWS_SECRET_ACCESS_KEY=${S3_SECRET_ACCESS_KEY}
      - AWS_REQUEST_CHECKSUM_CALCULATION=WHEN_REQUIRED
      - AWS_RESPONSE_CHECKSUM_VALIDATION=WHEN_REQUIRED  

    volumes:
      - /mnt/metrics:/mlflow/mlruns 
    command:
      - mlflow
      - server
      - --backend-store-uri=/mlflow/mlruns
      - --artifacts-destination=s3://{{ project_name }}-mlflow-artifacts
      - --serve-artifacts
      - --host=0.0.0.0
      - --port=8000
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15cd3155",
   "metadata": {},
   "source": [
    "# Fine-tuned model demo\n",
    "\n",
    "This notebook loads the base causal LM, applies a LoRA adapter (PEFT) saved from experiement, and runs a short question to generate answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202c9b0d",
   "metadata": {},
   "source": [
    "## import depedencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ef942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch, traceback\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19920876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs \n",
    "MODEL_NAME = \"Open-Orca/Mistral-7B-OpenOrca\"\n",
    "ADAPTER_DIR = \"/home/jovyan/data/adapters/adapter_final\"   # where we saved our adapters\n",
    "MAX_NEW_TOKENS = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d3521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer (fast if possible)\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "    print(\"Loaded fast tokenizer\")\n",
    "except Exception as e:\n",
    "    print(\"Fast tokenizer failed, falling back to slow. Error:\", e)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "    print(\"Loaded slow tokenizer\")\n",
    "\n",
    "# Ensure pad token exists\n",
    "if tokenizer.pad_token is None:\n",
    "    if getattr(tokenizer, \"eos_token\", None) is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        print(\"Set pad_token = eos_token (ok for causal LM).\")\n",
    "    else:\n",
    "        tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "        print(\"Added pad_token '<pad>' to tokenizer. Will resize embeddings after model load if needed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b44a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model (attempt multi-GPU auto placement; fallback to CPU)\n",
    "base = None\n",
    "try:\n",
    "    print(\"Loading base model with device_map='auto' (may take a while)...\")\n",
    "    base = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\", low_cpu_mem_usage=True, dtype=torch.float16)\n",
    "    print(\"Base model loaded with device_map='auto'.\")\n",
    "except Exception as e_auto:\n",
    "    print(\"device_map='auto' load failed:\", e_auto)\n",
    "    try:\n",
    "        print(\"Falling back to CPU load (this may be slower but safer)...\")\n",
    "        base = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"cpu\")\n",
    "        print(\"Base model loaded on CPU.\")\n",
    "    except Exception as e_cpu:\n",
    "        print(\"Failed to load base model. Traceback:\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "# Resize embeddings if tokenizer was modified\n",
    "if tokenizer.vocab_size != base.get_input_embeddings().weight.size(0):\n",
    "    try:\n",
    "        base.resize_token_embeddings(len(tokenizer))\n",
    "        print(\"Resized model embeddings to tokenizer length:\", len(tokenizer))\n",
    "    except Exception as e:\n",
    "        print(\"Warning: resize_token_embeddings failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c542e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA adapter\n",
    "if not Path(ADAPTER_DIR).exists():\n",
    "    raise FileNotFoundError(f\"Adapter directory not found: {ADAPTER_DIR}\")\n",
    "\n",
    "print(\"Applying LoRA adapter from:\", ADAPTER_DIR)\n",
    "try:\n",
    "    model = PeftModel.from_pretrained(base, ADAPTER_DIR)\n",
    "    print(\"Adapter applied (PeftModel).\")\n",
    "except Exception as e:\n",
    "    print(\"PeftModel.from_pretrained failed; attempting fallback: load adapter on CPU then move to GPU.\")\n",
    "    traceback.print_exc()\n",
    "    base_cpu = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"cpu\")\n",
    "    model = PeftModel.from_pretrained(base_cpu, ADAPTER_DIR)\n",
    "    # move to GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to(\"cuda\")\n",
    "    print(\"Fallback adapter load done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d69127",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# determine device where model params live (could be sharded)\n",
    "try:\n",
    "    model_device = next(model.parameters()).device\n",
    "except StopIteration:\n",
    "    model_device = torch.device(\"cpu\")\n",
    "print(\"Model parameters device:\", model_device)\n",
    "\n",
    "# simple prompt (edit as desired)\n",
    "prompt = \"Question: What methods and tools are used to assess the land-use suitability in the Kuala Terengganu coastal zone, and how does this information support urban planning and decision-making?\\nAnswer: \"\n",
    "\n",
    "# tokenize and move inputs to same device as model weights\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(model_device) for k, v in inputs.items()}\n",
    "\n",
    "# generate\n",
    "print(\"Generating...\")\n",
    "with torch.no_grad():\n",
    "    generated = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=False,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "# decode only the newly generated tokens\n",
    "generated_text = tokenizer.decode(generated[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "print(\"###### GENERATED ######\")\n",
    "print(generated_text.strip())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
